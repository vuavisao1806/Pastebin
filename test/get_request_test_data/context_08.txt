{"pasteValue":"At any instant during the period after an event arrives but before the system’s response to it is complete, either the system is working to respond to that event or the processing is blocked for some reason. This leads to the two basic contributors to the response time and resource usage: processing time (when the system is working to respond and actively consuming resources) and blocked time (when the system is unable to respond).   Processing time and resource usage. Processing consumes resources, which takes time. Events are handled by the execution of one or more components, whose time expended is a resource. Hardware resources include CPU, data stores, network communication bandwidth, and memory. Software resources include entities defined by the system under design. For example, thread pools and buffers must be managed and access to critical sections must be made sequential. For example, suppose a message is generated by one component. It might be placed on the network, after which it arrives at another component. It is then placed in a buffer; transformed in some fashion; processed according to some algorithm; transformed for output; placed in an output buffer; and sent onward to some component, another system, or some actor. Each of these steps contributes to the overall latency and resource con sumption of the processing of that event. Different resources behave differently as their utilization approaches their capacity— that is, as they become saturated. For example, as a CPU becomes more heavily loaded, performance usually degrades fairly steadily. In contrast, when you start to run out of memory, at some point the page swapping becomes overwhelming and performance crashes suddenly. Blocked time and resource contention. A computation can be blocked because of con tention for some needed resource, because the resource is unavailable, or because the computation depends on the result of other computations that are not yet available:  Contention for resources. Many resources can be used by only a single client at a time. As a consequence, other clients must wait for access to those resources. Figure 9.2 9.2 Tactics for Performance 139 shows events arriving at the system. These events may be in a single stream or in mul tiple streams. Multiple streams vying for the same resource or different events in the same stream vying for the same resource contribute to latency. The more contention for a resource that occurs, the more latency grows.   Availability of resources. Even in the absence of contention, computation cannot pro ceed if a resource is unavailable. Unavailability may be caused by the resource being offline or by failure of the component for any reason. Dependency on other computation. A computation may have to wait because it must synchronize with the results of another computation or because it is waiting for the results of a computation that it initiated. If a component calls another component and must wait for that component to respond, the time can be significant when the called component is at the other end of a network (as opposed to co-located on the same pro cessor), or when the called component is heavily loaded. Whatever the cause, you must identify places in the architecture where resource limitations might cause a significant contribution to overall latency. With this background, we turn to our tactic categories. We can either reduce demand for resources (control resource demand) or make the resources we have available handle the demand more effectively (manage resources). Control Resource Demand One way to increase performance is to carefully manage the demand for resources. This can be done by reducing the number of events processed or by limiting the rate at which the sys tem responds to events. In addition, a number of techniques can be applied to ensure that the resources that you do have are applied judiciously:  Manage work requests. One way to reduce work is to reduce the number of requests coming into the system to do work. Ways to do that include the following:   Manage event arrival. A common way to manage event arrivals from an external system is to put in place a service level agreement (SLA) that specifies the maximum event arrival rate that you are willing to support. An SLA is an agreement of the form “The system or component will process X events arriving per unit time with a response time of Y.” This agreement constrains both the system—it must provide that response—and the client—if it makes more than X requests per unit time, the response is not guaran teed. Thus, from the client’s perspective, if it needs more than X requests per unit time to be serviced, it must utilize multiple instances of the element processing the requests. SLAs are one method for managing scalability for Internet-based systems. Manage sampling rate. In cases where the system cannot maintain adequate response levels, you can reduce the sampling frequency of the stimuli—for example, the rate at which data is received from a sensor or the number of video frames per second that you process. Of course, the price paid here is the fidelity of the video stream or the information you gather from the sensor data. Nevertheless, this is a viable strategy if the result is “good enough.” Such an approach is commonly used in signal processing 140 Part II Quality Attributes | Chapter 9 Performance systems where, for example, different codices can be chosen with different sampling rates and data formats. This design choice seeks to maintain predictable levels of latency; you must decide whether having a lower fidelity but consistent stream of data is preferable to having erratic latency. Some systems manage the sampling rate dynam ically in response to latency measures or accuracy needs.    Limit event response. When discrete events arrive at the system (or component) too rapidly to be processed, then the events must be queued until they can be processed, or they are simply discarded. You may choose to process events only up to a set maximum rate, thereby ensuring predictable processing for the events that are actually processed. This tactic could be triggered by a queue size or processor utilization exceeding some warning level. Alternatively, it could be triggered by an event rate that violates an SLA. If you adopt this tactic and it is unacceptable to lose any events, then you must ensure that your queues are large enough to handle the worst case. Conversely, if you choose to drop events, then you need to choose a policy: Do you log the dropped events or simply ignore them? Do you notify other systems, users, or administrators? Prioritize events. If not all events are equally important, you can impose a priority scheme that ranks events according to how important it is to service them. If insufficient resources are available to service them when they arise, low-priority events might be ignored. Ignoring events consumes minimal resources (including time), thereby increas ing performance compared to a system that services all events all the time. For example, a building management system may raise a variety of alarms. Life-threatening alarms such as a fire alarm should be given higher priority than informational alarms such as a room being too cold. Reduce computational overhead. For events that do make it into the system, the follow ing approaches can be implemented to reduce the amount of work involved in handling each event:   Reduce indirection. The use of intermediaries (so important for modifiability, as we saw in Chapter 8) increases the computational overhead in processing an event stream, so removing them improves latency. This is a classic modifiability/performance tradeoff. Separation of concerns—another linchpin of modifiability—can also increase the pro cessing overhead necessary to service an event if it leads to an event being serviced by a chain of components rather than a single component. You may be able to realize the best of both worlds, however: Clever code optimization can let you program using the intermediaries and interfaces that support encapsulation (and thus keep the modifiabil ity) but reduce, or in some cases eliminate, the costly indirection at runtime. Similarly, some brokers allow for direct communication between a client and a server (after ini tially establishing the relationship via the broker), thereby eliminating the indirection step for all subsequent requests. Co-locate communicating resources. Context switching and intercomponent com munication costs add up, especially when the components are on different nodes on a network. One strategy for reducing computational overhead is to co-locate resources. Co-location may mean hosting cooperating components on the same processor to avoid 9.2 Tactics for Performance 141   the time delay of network communication; it may mean putting the resources in the same runtime software component to avoid even the expense of a subroutine call; or it may mean placing tiers of a multi-tier architecture on the same rack in the data center. Periodic cleaning. A special case when reducing computational overhead is to perform a periodic cleanup of resources that have become inefficient. For example, hash tables and virtual memory maps may require recalculation and reinitialization. Many system administrators and even regular computer users do a periodic reboot of their systems for exactly this reason. Bound execution times. You can place a limit on how much execution time is used to respond to an event. For iterative, data-dependent algorithms, limiting the number of iterations is a method for bounding execution times. The cost, however, is usually a less accurate computation. If you adopt this tactic, you will need to assess its effect on accuracy and see if the result is “good enough.” This resource management tactic is frequently paired with the manage sampling rate tactic.  Increase efficiency of resource usage. Improving the efficiency of algorithms used in critical areas can decrease latency and improve throughput and resource consumption. This is, for some programmers, their primary performance tactic. If the system does not perform adequately, they try to “tune up” their processing logic. As you can see, this approach is actually just one of many tactics available. Manage Resources Even if the demand for resources is not controllable, the management of these resources can be. Sometimes one resource can be traded for another. For example, intermediate data may be kept in a cache or it may be regenerated depending on which resources are more critical: time, space, or network bandwidth. Here are some resource management tactics:    Increase resources. Faster processors, additional processors, additional memory, and faster networks all have the potential to improve performance. Cost is usually a con sideration in the choice of resources, but increasing the resources is, in many cases, the cheapest way to get immediate improvement. Introduce concurrency. If requests can be processed in parallel, the blocked time can be reduced. Concurrency can be introduced by processing different streams of events on different threads or by creating additional threads to process different sets of activities. (Once concurrency has been introduced, you can choose scheduling policies to achieve the goals you find desirable using the schedule resources tactic.) Maintain multiple copies of computations. This tactic reduces the contention that would occur if all requests for service were allocated to a single instance. Replicated services in a microservice architecture or replicated web servers in a server pool are examples of replicas of computation. A load balancer is a piece of software that assigns new work to one of the available duplicate servers; criteria for assignment vary but can be as simple as a round-robin scheme or assigning the next request to the least busy server. The load balancer pattern is discussed in detail in Section 9.4. 142 Part II Quality Attributes | Chapter 9 Performance    Maintain multiple copies of data. Two common examples of maintaining multiple copies of data are data replication and caching. Data replication involves keeping separate copies of the data to reduce the contention from multiple simultaneous accesses. Because the data being replicated is usually a copy of existing data, keeping the copies consistent and synchronized becomes a responsibility that the system must assume. Caching also involves keeping copies of data (with one set of data possibly being a subset of the other), but on storage with different access speeds. The different access speeds may be due to memory speed versus secondary storage speed, or the speed of local versus remote communication. Another responsibility with caching is choosing the data to be cached. Some caches oper ate by merely keeping copies of whatever was recently requested, but it is also possible to predict users’ future requests based on patterns of behavior, and to begin the calculations or prefetches necessary to comply with those requests before the user has made them. Bound queue sizes. This tactic controls the maximum number of queued arrivals and consequently the resources used to process the arrivals. If you adopt this tactic, you need to establish a policy for what happens when the queues overflow and decide if not responding to lost events is acceptable. This tactic is frequently paired with the limit event response tactic. Schedule resources. Whenever contention for a resource occurs, the resource must be scheduled. Processors are scheduled, buffers are scheduled, and networks are sched uled. Your concern as an architect is to understand the characteristics of each resource’s use and choose the scheduling strategy that is compatible with it. (See the “Scheduling Policies” sidebar.) Figure 9.3 summarizes the tactics for performance","expiryTime":"never","title":"who are you"}